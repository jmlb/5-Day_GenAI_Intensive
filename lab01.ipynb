{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31ecf2b7-e479-44ba-a504-a069b912cb0a",
   "metadata": {},
   "source": [
    "# Lab01 using openAI library\n",
    "\n",
    "https://developers.googleblog.com/en/gemini-is-now-accessible-from-the-openai-library/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5aa57cf3-ed58-46f4-a154-6a98d4c77063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries version\n",
    "# genai.__version__ == 0.8.3\n",
    "# openai.__version__ == 1.54.4\n",
    "# langchain.__version__ == 0.3.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c510fd1-659d-439b-b9bc-041f53e62fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import google.generativeai as genai  \n",
    "from openai import OpenAI\n",
    "from IPython.display import HTML, Markdown, display\n",
    "\n",
    "# Load the variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the variables\n",
    "gemini_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=gemini_api_key,\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/\"\n",
    ")\n",
    "\n",
    "\n",
    "def ask_model(msgs, llm, **kwargs):\n",
    "    response = client.chat.completions.create(\n",
    "    model=llm,\n",
    "    n=1,\n",
    "    messages=msgs,\n",
    "    **kwargs\n",
    "    )\n",
    "    return response.choices[0].message\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4133c227-c639-4fff-bfd4-8be4276709cd",
   "metadata": {},
   "source": [
    "# 1. Example: api call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f75ceff-5fa7-499a-97c7-7f2a0a52fd0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Imagine you have a really smart friend who loves to learn new things. That friend is like AI, or Artificial Intelligence. \n",
       "\n",
       "AI is like a computer program that can learn and solve problems just like you can! It can learn from lots of information, like reading books or watching videos, and then use that information to answer questions, play games, or even write stories!\n",
       "\n",
       "Think about how you learn to ride a bike: you practice and get better over time. AI is the same way. It keeps practicing and gets better at what it does, like recognizing your face in a photo or playing your favorite video game.\n",
       "\n",
       "AI is already helping us in lots of ways. It can translate languages, tell you what the weather will be like, and even help doctors diagnose illnesses. \n",
       "\n",
       "But AI is still learning and growing, just like you!  It's exciting to see what amazing things AI will be able to do in the future! \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flash = \"gemini-1.5-flash\"\n",
    "\n",
    "user_prompt = \"Explain AI to me like I'm a kid.\"\n",
    "messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "\n",
    "answer = ask_model(messages, flash)\n",
    "\n",
    "Markdown(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaac3f0-653e-446f-a714-fb3f7163ffeb",
   "metadata": {},
   "source": [
    "# 2. Multi-Turn chat and history tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a08d56ae-0b3f-4689-88ea-0acf93f91ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Prompt: Hello! My name is Zlork.\n",
      "Assistant: \n",
      "Hello Zlork! It's nice to meet you. ðŸ˜Š What can I do for you today? \n",
      "\n",
      "--\n",
      "User Prompt: Can you tell something interesting about dinosaurs?\n",
      "Assistant: \n",
      "You're in for a treat, Zlork! Dinosaurs are full of fascinating facts.  Here's one: \n",
      "\n",
      "Did you know that some dinosaurs had feathers?  It's true! We used to think they were all scaly, but fossils have shown that many species, like Velociraptor and T-Rex, had feathers, although some were more like downy fuzz than the flight feathers of birds today. \n",
      "\n",
      "This discovery tells us that dinosaurs were likely more bird-like than we thought, and it gives us a glimpse into their evolution. What else would you like to know about dinosaurs? \n",
      "\n",
      "--\n",
      "User Prompt: Do you remember what my name is?\n",
      "Assistant: \n",
      "Of course I remember! You're Zlork.  ðŸ˜Š I'm a pretty good listener, especially when it comes to names.  Is there anything else I can help you with, Zlork? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize conversation history\n",
    "conversation_history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "\n",
    "def chat_with_openai(prompt, history, model, **kwargs):\n",
    "    # Add user's message to the history\n",
    "    history.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    response = ask_model(history, model, **kwargs)\n",
    "    \n",
    "    # Get the assistant's response\n",
    "    assistant_message = response.content\n",
    "    \n",
    "    # Add assistant's message to the history\n",
    "    history.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "    \n",
    "    return assistant_message, history\n",
    "\n",
    "\n",
    "# First user prompt\n",
    "user_prompt = 'Hello! My name is Zlork.'\n",
    "response, conversation_history = chat_with_openai(user_prompt, conversation_history, flash)\n",
    "print(f\"User Prompt: {user_prompt}\")\n",
    "print(f\"Assistant: \\n{response}\")\n",
    "print(\"--\")\n",
    "# Second user prompt\n",
    "user_prompt = \"Can you tell something interesting about dinosaurs?\"\n",
    "response, conversation_history = chat_with_openai(user_prompt, conversation_history, flash)\n",
    "print(f\"User Prompt: {user_prompt}\")\n",
    "print(f\"Assistant: \\n{response}\")\n",
    "print(\"--\")\n",
    "# Third user prompt\n",
    "user_prompt = \"Do you remember what my name is?\"\n",
    "response, conversation_history = chat_with_openai(user_prompt, conversation_history, flash)\n",
    "print(f\"User Prompt: {user_prompt}\")\n",
    "print(f\"Assistant: \\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e6e568-95ce-44bf-b298-a5a85ad500a8",
   "metadata": {},
   "source": [
    "# 3. Choose a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d285f28-6113-4c58-b3d4-dceedba4c479",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m genai\u001b[38;5;241m.\u001b[39mlist_models():\n\u001b[1;32m      2\u001b[0m   \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[0;32m~/py310env/lib/python3.10/site-packages/google/generativeai/models.py:204\u001b[0m, in \u001b[0;36mlist_models\u001b[0;34m(page_size, client, request_options)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     client \u001b[38;5;241m=\u001b[39m get_default_model_client()\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpage_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    205\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(model)\u001b[38;5;241m.\u001b[39mto_dict(model)\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m model_types\u001b[38;5;241m.\u001b[39mModel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel)\n",
      "File \u001b[0;32m~/py310env/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/model_service/client.py:916\u001b[0m, in \u001b[0;36mModelServiceClient.list_models\u001b[0;34m(self, request, page_size, page_token, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m    915\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 916\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# This method is paged; wrap the response in a pager, which provides\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# an `__iter__` convenience method.\u001b[39;00m\n\u001b[1;32m    925\u001b[0m response \u001b[38;5;241m=\u001b[39m pagers\u001b[38;5;241m.\u001b[39mListModelsPager(\n\u001b[1;32m    926\u001b[0m     method\u001b[38;5;241m=\u001b[39mrpc,\n\u001b[1;32m    927\u001b[0m     request\u001b[38;5;241m=\u001b[39mrequest,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    931\u001b[0m     metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[1;32m    932\u001b[0m )\n",
      "File \u001b[0;32m~/py310env/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/py310env/lib/python3.10/site-packages/google/api_core/retry.py:372\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    369\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    371\u001b[0m )\n\u001b[0;32m--> 372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/py310env/lib/python3.10/site-packages/google/api_core/retry.py:207\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 207\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    209\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[0;32m~/py310env/lib/python3.10/site-packages/google/api_core/timeout.py:120\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Avoid setting negative timeout\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m-\u001b[39m time_since_first_attempt)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/py310env/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:79\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(callable_)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror_remapped_callable\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[0;32m~/py310env/lib/python3.10/site-packages/grpc/_channel.py:1178\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1168\u001b[0m     request: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1173\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1174\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1175\u001b[0m     (\n\u001b[1;32m   1176\u001b[0m         state,\n\u001b[1;32m   1177\u001b[0m         call,\n\u001b[0;32m-> 1178\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_blocking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/py310env/lib/python3.10/site-packages/grpc/_channel.py:1162\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._blocking\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1145\u001b[0m state\u001b[38;5;241m.\u001b[39mtarget \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target)\n\u001b[1;32m   1146\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channel\u001b[38;5;241m.\u001b[39msegregated_call(\n\u001b[1;32m   1147\u001b[0m     cygrpc\u001b[38;5;241m.\u001b[39mPropagationConstants\u001b[38;5;241m.\u001b[39mGRPC_PROPAGATE_DEFAULTS,\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registered_call_handle,\n\u001b[1;32m   1161\u001b[0m )\n\u001b[0;32m-> 1162\u001b[0m event \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1163\u001b[0m _handle_event(event, state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_deserializer)\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:388\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:213\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/threading.py:264\u001b[0m, in \u001b[0;36mCondition.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39m_at_fork_reinit()\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_waiters\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m--> 264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for model in genai.list_models():\n",
    "  print(model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed9fd29-2168-4d8e-b7bb-cd8b90fba9b7",
   "metadata": {},
   "source": [
    "# 4. Model details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d3196bdc-c4cb-436a-8f11-cbbe9b45e70c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[142], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m genai\u001b[38;5;241m.\u001b[39mlist_models():\n\u001b[1;32m      2\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/gemini-1.5-flash\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(model)\n",
      "File \u001b[0;32m~/py310env/lib/python3.10/site-packages/google/generativeai/models.py:204\u001b[0m, in \u001b[0;36mlist_models\u001b[0;34m(page_size, client, request_options)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     client \u001b[38;5;241m=\u001b[39m get_default_model_client()\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpage_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    205\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(model)\u001b[38;5;241m.\u001b[39mto_dict(model)\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m model_types\u001b[38;5;241m.\u001b[39mModel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel)\n",
      "File \u001b[0;32m~/py310env/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/model_service/client.py:916\u001b[0m, in \u001b[0;36mModelServiceClient.list_models\u001b[0;34m(self, request, page_size, page_token, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m    915\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 916\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# This method is paged; wrap the response in a pager, which provides\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# an `__iter__` convenience method.\u001b[39;00m\n\u001b[1;32m    925\u001b[0m response \u001b[38;5;241m=\u001b[39m pagers\u001b[38;5;241m.\u001b[39mListModelsPager(\n\u001b[1;32m    926\u001b[0m     method\u001b[38;5;241m=\u001b[39mrpc,\n\u001b[1;32m    927\u001b[0m     request\u001b[38;5;241m=\u001b[39mrequest,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    931\u001b[0m     metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[1;32m    932\u001b[0m )\n",
      "File \u001b[0;32m~/py310env/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/py310env/lib/python3.10/site-packages/google/api_core/retry.py:372\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    369\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    371\u001b[0m )\n\u001b[0;32m--> 372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/py310env/lib/python3.10/site-packages/google/api_core/retry.py:236\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mRetryError(\n\u001b[1;32m    227\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeadline of \u001b[39m\u001b[38;5;132;01m{:.1f}\u001b[39;00m\u001b[38;5;124ms exceeded while calling target function\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    228\u001b[0m                     timeout\n\u001b[1;32m    229\u001b[0m                 ),\n\u001b[1;32m    230\u001b[0m                 last_exc,\n\u001b[1;32m    231\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlast_exc\u001b[39;00m\n\u001b[1;32m    233\u001b[0m     _LOGGER\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    234\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying due to \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, sleeping \u001b[39m\u001b[38;5;132;01m{:.1f}\u001b[39;00m\u001b[38;5;124ms ...\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(last_exc, sleep)\n\u001b[1;32m    235\u001b[0m     )\n\u001b[0;32m--> 236\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSleep generator stopped yielding sleep values.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for model in genai.list_models():\n",
    "  if model.name == 'models/gemini-1.5-flash':\n",
    "    print(model)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b598723e-ec17-4a7d-9d4f-6acc55ac8ddd",
   "metadata": {},
   "source": [
    "# 5. Explore generation parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384b77b9-90b9-43ac-a9d9-33cc077a067a",
   "metadata": {},
   "source": [
    "## 5.1 number of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d07e431d-d0e6-48c1-9765-0eb96713c101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Beyond the Brine: The Enduring Importance of Olives in Modern Society\n",
      "\n",
      "The olive, a small, unassuming fruit, holds a surprising amount of significance in modern society. Beyond its simple appearance lies a history rich in culture, tradition, and economic value. From the Mediterranean shores where it originated to the globalized markets where it is now enjoyed, the olive has woven itself into the fabric of human civilization, playing a critical role in nutrition, economy, and even environmental sustainability.\n",
      "\n",
      "Perhaps the most recognized aspect of the olive's importance is its role in the human diet. For millennia, olives have been a staple food in the Mediterranean region, contributing to the famed \"Mediterranean diet\" that is now lauded for its health benefits. Olives are rich in monounsaturated fats, particularly oleic acid, which has been linked to heart health and a reduced risk of chronic diseases. They also contain valuable antioxidants, vitamins, and minerals, contributing to overall well-being. \n",
      "\n",
      "Beyond their nutritional\n"
     ]
    }
   ],
   "source": [
    "model = \"gemini-1.5-flash\"\n",
    "\n",
    "conversation_history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "]\n",
    "user_prompt = \"Write a 1000 word essay on the importance of olives in modern society.\"\n",
    "\n",
    "response, _ = chat_with_openai(user_prompt, conversation_history, model, max_tokens=200)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e025e7ff-5468-4a5f-9e0b-1891f9e2d0bb",
   "metadata": {},
   "source": [
    "## 5.2 Temperature\n",
    "\n",
    "OpenAI's Python API client does not include a built-in retry mechanism like google.api_core. \n",
    "However, you can implement a retry mechanism yourself using Python's tenacity library, \n",
    "which is commonly used for handling retries with customizable policies.\n",
    "!pip install tenacity\n",
    "\n",
    "- *retry_if_exception_type*: Retries only if specific exceptions occur, such as RateLimitError, APIConnectionError, or Timeout.\n",
    "\n",
    "- *stop_after_attempt(5)*: Stops retrying after 5 attempts.\n",
    "\n",
    "- *wait_exponential*: Implements exponential backoff with a base of 4 seconds, doubling each retry, up to a maximum of 60 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8648b766-a955-4648-874e-c0f0f5e0760f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "from openai import RateLimitError, APIConnectionError, Timeout, OpenAIError\n",
    "\n",
    "# Define a retry strategy\n",
    "@retry(\n",
    "    retry=retry_if_exception_type((RateLimitError, APIConnectionError, Timeout)),\n",
    "    stop=stop_after_attempt(5),  # Retry up to 5 times\n",
    "    wait=wait_exponential(multiplier=1, min=4, max=60),  # Exponential backoff\n",
    ")\n",
    "def chat_with_openai_with_retry(prompt, history, model, **kwargs):\n",
    "    response, history = chat_with_openai(prompt, history, model, **kwargs)\n",
    "    try:\n",
    "        response, history = chat_with_openai(prompt, history, model, **kwargs)\n",
    "        return response, history\n",
    "    except OpenAIError as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        raise  # Re-raise the error to trigger the retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc78a604-5bad-46a5-8981-8e60d47daa6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purple \n",
      " -------------------------\n",
      "Teal \n",
      " -------------------------\n",
      "Azure \n",
      " -------------------------\n",
      "Teal. \n",
      " -------------------------\n",
      "Purple \n",
      " -------------------------\n",
      "Teal \n",
      " -------------------------\n"
     ]
    }
   ],
   "source": [
    "# higher Temperature\n",
    "\n",
    "model = \"gemini-1.5-flash\"\n",
    "temperature = 2\n",
    "trial = 0\n",
    "\n",
    "while trial <= 5:\n",
    "    try:\n",
    "        user_prompt = \"Pick a random colour... (respond in a single word)\"\n",
    "        conversation_history = []\n",
    "        response, _ = chat_with_openai_with_retry(user_prompt, conversation_history, model, temperature=temperature)\n",
    "        print(response, '-' * 25)\n",
    "        trial += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Final exception after retries: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d0812ed-eded-4cc8-bb0d-38f231693c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teal \n",
      " -------------------------\n",
      "Teal \n",
      " -------------------------\n",
      "Teal \n",
      " -------------------------\n",
      "Teal \n",
      " -------------------------\n",
      "Teal \n",
      " -------------------------\n",
      "Teal \n",
      " -------------------------\n"
     ]
    }
   ],
   "source": [
    "# Low temp\n",
    "\n",
    "model = \"gemini-1.5-flash\"\n",
    "temperature = 0\n",
    "\n",
    "trial = 0\n",
    "\n",
    "while trial <= 5:\n",
    "    try:\n",
    "        user_prompt = \"Pick a random colour... (respond in a single word)\"\n",
    "        conversation_history = []\n",
    "        response, _ = chat_with_openai_with_retry(user_prompt, conversation_history, model, temperature=temperature)\n",
    "        print(response, '-' * 25)\n",
    "        trial += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Final exception after retries: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c54457-0597-4882-9b5c-dbb02c837081",
   "metadata": {},
   "source": [
    "## 5.3 Top-K and top-P\n",
    "\n",
    "Like temperature, top-K and top-P parameters are also used to control the diversity of the model's output.\n",
    "\n",
    "Top-K is a positive integer that defines the number of most probable tokens from which to select the output token. A top-K of 1 selects a single token, performing greedy decoding.\n",
    "\n",
    "Top-P defines the probability threshold that, once cumulatively exceeded, tokens stop being selected as candidates. A top-P of 0 is typically equivalent to greedy decoding, and a top-P of 1 typically selects every token in the model's vocabulary.\n",
    "\n",
    "When both are supplied, the Gemini API will filter top-K tokens first, then top-P and then finally sample from the candidate tokens using the supplied temperature.\n",
    "\n",
    "Run this example a number of times, change the settings and observe the change in output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f21420d2-2574-46f1-9a62-337fa3f4bfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Humphrey was a cat of routine. He woke at 6:30 am, demanded breakfast, napped in the sunbeam on the living room rug, and then proceeded to chase imaginary foes in the garden until dinner. This was his life, his world, his joy â€“ a life of predictable, fluffy bliss. \n",
      "\n",
      "But one day, the wind changed. A gust, strong and sudden, ripped open the back door, sending Humphrey tumbling onto the patio. He landed with a surprised meow, his whiskers twitching, and found himself staring atâ€¦ a box. Not just any box, mind you, a cardboard box, overflowing with colorful scarves. \n",
      "\n",
      "This was not in Humphreyâ€™s routine. Curiosity, that insatiable feline itch, began to prickle. He sniffed the box cautiously, then poked his head inside. The scent of adventure, a heady mix of silk and faraway places, washed over him. He jumped in, his paws landing on a riot of colors.\n",
      "\n",
      "The scarves, he discovered, were his ticket to another world. He squeezed between silk flowers, his whiskers brushing against smooth satin. He chased the dancing shadows cast by the sun through the fabric. He even tried to climb a rainbow scarf, finding it surprisingly supportive, much to his delight. \n",
      "\n",
      "He spent the afternoon exploring, weaving through fabrics, scaling silk mountains, and hiding in the folds of a velvet shawl. It was a world of texture and scent, unlike anything heâ€™d encountered before. \n",
      "\n",
      "As evening settled, Humphrey emerged from his scarf-filled wonderland, his fur ruffled, his eyes shining with a newfound sparkle. He sat, basking in the fading light, a satisfied purr rumbling in his chest. \n",
      "\n",
      "He may have returned to his routine, but his life was forever changed. The world, he realized, was full of unexpected wonders, hidden just beyond the familiar. And Humphrey, the cat of routine, was ready to find them. He knew, with a certainty that only a cat who'd traveled through a box of scarves could possess, that adventures were waiting, just around the corner. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# openai library does not support top_k directly\n",
    "\n",
    "user_prompt = \"Write a short story about a cat who goes on an adventure.\"\n",
    "conversation_history = [\n",
    "                        {\"role\": \"system\", \"content\": \"You are a creative writer.\"}\n",
    "        ]\n",
    "params = dict(top_p=0.95, temperature=1.0)\n",
    "response, _ = chat_with_openai_with_retry(user_prompt, \n",
    "                                          conversation_history, \n",
    "                                          model, \n",
    "                                          **params)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca0cb2f-6ae4-4b14-bc97-519b2c99f23e",
   "metadata": {},
   "source": [
    "# 6. Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa827ca-e025-4497-8a11-4e5173543684",
   "metadata": {},
   "source": [
    "# 6.1 Zero-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71be1f9c-c0a9-4c4b-b61d-9cc1f5c4c68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: **POSITIVE**\n"
     ]
    }
   ],
   "source": [
    "# We remove the system role entirely, because the instruction is included in the user's prompt, \n",
    "# and the model only needs to respond to the classification.\n",
    "\n",
    "conversation_history = []\n",
    "params = dict(top_p=1, temperature=0.1, max_tokens=5)\n",
    "\n",
    "zero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
    "Review: \"Her\" is a disturbing study revealing the direction\n",
    "humanity is headed if AI is allowed to keep evolving,\n",
    "unchecked. I wish there were more movies like this masterpiece.\n",
    "Sentiment: \"\"\"\n",
    "\n",
    "response, _ = chat_with_openai_with_retry(zero_shot_prompt, \n",
    "                                          conversation_history, \n",
    "                                          model, \n",
    "                                          **params)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41440102-d367-4fc4-9bac-a4950cfe116f",
   "metadata": {},
   "source": [
    "## 6.2 One shot and few shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10ed2c77-80ad-4ade-8526-500ef6c0903b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\"size\": \"large\",\n",
      "\"type\": \"normal\",\n",
      "\"ingredients\": [\"cheese\", \"pineapple\"]\n",
      "}\n",
      "``` \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "\n",
    "params = dict(top_p=1, temperature=0.1, max_tokens=250)\n",
    "\n",
    "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
    "EXAMPLE:\n",
    "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"small\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"cheese\", \"tomato sauce\", \"peperoni\"]\n",
    "}\n",
    "```\n",
    "\n",
    "EXAMPLE:\n",
    "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"large\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
    "}\n",
    "\n",
    "ORDER:\n",
    "\"\"\"\n",
    "\n",
    "conversation_history = [{\"role\": \"system\", \"content\": dedent(few_shot_prompt)}]\n",
    "customer_order = \"Give me a large with cheese & pineapple\"\n",
    "\n",
    "response, _ = chat_with_openai_with_retry(customer_order, \n",
    "                                          conversation_history, \n",
    "                                          model, \n",
    "                                          **params)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee41f06d-ba63-46f8-bbae-f8a323982a6a",
   "metadata": {},
   "source": [
    "## JSON mode\n",
    "\n",
    "For simplicity we will use langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "244ebd87-8af1-4040-8bb7-66aa91869cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'PizzaOrder',\n",
       " 'description': 'A function to parse pizza order',\n",
       " 'parameters': {'properties': {'size': {'description': 'The size of the pizza',\n",
       "    'type': 'string'},\n",
       "   'ingredients': {'description': 'List of ingredients on the pizza',\n",
       "    'items': {'type': 'string'},\n",
       "    'type': 'array'},\n",
       "   'type': {'description': 'The type of pizza', 'type': 'string'}},\n",
       "  'required': ['size', 'ingredients', 'type'],\n",
       "  'type': 'object'}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.utils.function_calling import convert_pydantic_to_openai_function\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "pizza_order_tool "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "187c1354-ab45-4561-b64f-93691866a489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw response  ChatCompletion(id=None, choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='model', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='0', function=Function(arguments='{\"type\":\"dessert\",\"size\":\"large\",\"ingredients\":[\"apple\",\"chocolate\"]}', name='PizzaOrder'), type='function')]))], created=1731553446, model='gemini-1.5-flash', object='chat.completion', service_tier=None, system_fingerprint=None, usage=None)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.utils.function_calling import convert_pydantic_to_openai_function\n",
    "from pydantic import BaseModel, Field\n",
    "import json\n",
    "\n",
    "\n",
    "class PizzaOrder(BaseModel):\n",
    "    \"\"\"A function to parse pizza order\"\"\"\n",
    "    size: str = Field(description=\"The size of the pizza\")\n",
    "    ingredients: list[str] = Field(description=\"List of ingredients on the pizza\")\n",
    "    type: str = Field(description=\"The type of pizza\")\n",
    "\n",
    "\n",
    "pizza_order_tool = convert_pydantic_to_openai_function(PizzaOrder)\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Can I have a large dessert pizza with apple and chocolate\"}],\n",
    "    tools=[{\"type\": \"function\", \"function\": pizza_order_tool}]\n",
    ")\n",
    "\n",
    "print(\"raw response \", response) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "32f72651-fdf6-4ade-8786-00e669ef0e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"type\": \"dessert\",\n",
      "    \"size\": \"large\",\n",
      "    \"ingredients\": [\n",
      "        \"apple\",\n",
      "        \"chocolate\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Json format response\n",
    "\n",
    "# Extract the tool call data from the response\n",
    "tool_call = response.choices[0].message.tool_calls[0]\n",
    "\n",
    "# Get the function call arguments and parse as JSON\n",
    "function_arguments = tool_call.function.arguments\n",
    "parsed_arguments = json.loads(function_arguments)\n",
    "\n",
    "# Print the resulting JSON\n",
    "print(json.dumps(parsed_arguments, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca744b81-800f-497c-b040-c21fb2b5ab2e",
   "metadata": {},
   "source": [
    "## 6.3 Chain Of Thought (CoT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6c9769dd-3c09-484d-bc69-384a88db2eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CoT:  52 \n",
      "\n",
      "----------------------------------------\n",
      "With CoT  Here's how to figure this out:\n",
      "\n",
      "* **Partner's age when you were 4:** Your partner was 3 times your age, so they were 4 * 3 = 12 years old.\n",
      "* **Age difference:** This means your partner is 12 - 4 = 8 years older than you.\n",
      "* **Partner's current age:**  Since you are now 20, your partner is 20 + 8 = **28 years old**. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = 'gemini-1.5-flash-latest'\n",
    "\n",
    "user_prompt_no_CoT = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
    "am 20 years old. How old is my partner? Return the answer immediately.\"\"\"\n",
    "\n",
    "conversation_history = []\n",
    "\n",
    "response, _ = chat_with_openai_with_retry(user_prompt_no_CoT, \n",
    "                                          conversation_history, \n",
    "                                          model)\n",
    "\n",
    "print(\"No CoT: \", response)\n",
    "print(\"--\"*20)\n",
    "\n",
    "user_prompt_with_CoT = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\n",
    "I am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n",
    "conversation_history = []\n",
    "response, _ = chat_with_openai_with_retry(user_prompt_with_CoT, \n",
    "                                       conversation_history, \n",
    "                                       model)\n",
    "print(\"With CoT \", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddbb777-7acd-4e5d-8934-63c273efabe0",
   "metadata": {},
   "source": [
    "## 6.4 ReAct: Reason and Act\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d377b163-51fd-452e-a18c-47c8801e0a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_instructions = \"\"\"\n",
    "Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\n",
    "Observation is understanding relevant information from an Action's output and Action can be one of three types:\n",
    " (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n",
    "     will return some similar entities to search and you can try to search the information from those topics.\n",
    " (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n",
    "     so keep your searches short.\n",
    " (3) <finish>answer</finish>, which returns the answer and finishes the task.\n",
    "\"\"\"\n",
    "\n",
    "example1 = \"\"\"Question\n",
    "Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
    "\n",
    "Thought 1\n",
    "The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n",
    "\n",
    "Action 1\n",
    "<search>Milhouse</search>\n",
    "\n",
    "Observation 1\n",
    "Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
    "\n",
    "Thought 2\n",
    "The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n",
    "\n",
    "Action 2\n",
    "<lookup>named after</lookup>\n",
    "\n",
    "Observation 2\n",
    "Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n",
    "\n",
    "Thought 3\n",
    "Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n",
    "\n",
    "Action 3\n",
    "<finish>Richard Nixon</finish>\n",
    "\"\"\"\n",
    "\n",
    "example2 = \"\"\"Question\n",
    "What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
    "\n",
    "Thought 1\n",
    "I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n",
    "\n",
    "Action 1\n",
    "<search>Colorado orogeny</search>\n",
    "\n",
    "Observation 1\n",
    "The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
    "\n",
    "Thought 2\n",
    "It does not mention the eastern sector. So I need to look up eastern sector.\n",
    "\n",
    "Action 2\n",
    "<lookup>eastern sector</lookup>\n",
    "\n",
    "Observation 2\n",
    "The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
    "\n",
    "Thought 3\n",
    "The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n",
    "\n",
    "Action 3\n",
    "<search>High Plains</search>\n",
    "\n",
    "Observation 3\n",
    "High Plains refers to one of two distinct land regions\n",
    "\n",
    "Thought 4\n",
    "I need to instead search High Plains (United States).\n",
    "\n",
    "Action 4\n",
    "<search>High Plains (United States)</search>\n",
    "\n",
    "Observation 4\n",
    "The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n",
    "\n",
    "Thought 5\n",
    "High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n",
    "\n",
    "Action 5\n",
    "<finish>1,800 to 7,000 ft</finish>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "73ab8fd4-3788-4b34-8d86-b554e2e28da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 1: I need to find the Transformers paper and identify the authors, then figure out who is the youngest.\n",
      "\n",
      "Action 1: <search>Transformers paper</search>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = 'gemini-1.5-flash-latest'\n",
    "\n",
    "question = \"\"\"Question\n",
    "Who was the youngest author listed on the transformers NLP paper?\n",
    "\"\"\"\n",
    "\n",
    "conversation_history = [{\"role\": \"user\", \"content\": dedent(model_instructions)},\n",
    "                        {\"role\": \"user\", \"content\": dedent(example1)},\n",
    "                        {\"role\": \"user\", \"content\": dedent(example2)},\n",
    "                       ]\n",
    "\n",
    "# stop: Takes a list of strings where the model should stop generating text.\n",
    "params= dict(stop=[\"\\nObservation\"])\n",
    "response, conversation_history = chat_with_openai_with_retry(dedent(question), \n",
    "                                          conversation_history, \n",
    "                                          model,\n",
    "                                         **params)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "66bb04e3-6b83-45a8-a781-35ea74c1f65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 2: I need to look for the authors and try to determine the youngest. Unfortunately, the paper itself does not give information on ages.\n",
      "\n",
      "Action 2: <search>Ashish Vaswani</search> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now you can perform this research yourself and supply it back to the model.\n",
    "\n",
    "observation = \"\"\"Observation 1\n",
    "[1706.03762] Attention Is All You Need\n",
    "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
    "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
    "\"\"\"\n",
    "\n",
    "response, conversation_history = chat_with_openai_with_retry(dedent(observation), \n",
    "                                          conversation_history, \n",
    "                                          model,\n",
    "                                         **params)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26ebf10-7d4c-48ba-8ffd-22ea4db4b53c",
   "metadata": {},
   "source": [
    "# 7 Code prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e149657b-69f4-4317-af07-021d9d4221e8",
   "metadata": {},
   "source": [
    "## 7.1 Generating code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "312f70db-ea43-4394-bc45-1606e6b8b8df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def factorial(n):\n",
       "  \"\"\"Calculates the factorial of a non-negative integer.\n",
       "\n",
       "  Args:\n",
       "    n: The non-negative integer for which to calculate the factorial.\n",
       "\n",
       "  Returns:\n",
       "    The factorial of n, or 1 if n is 0.\n",
       "  \"\"\"\n",
       "  if n < 0:\n",
       "    raise ValueError(\"Factorial is not defined for negative numbers.\")\n",
       "  elif n == 0:\n",
       "    return 1\n",
       "  else:\n",
       "    return n * factorial(n - 1)\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_history = [{\"role\": \"system\", \"content\": \"Act as a software developer. Stick to coding only!\"}]  # modified\n",
    "\n",
    "# Gemini 1.5 models are very chatty, so it helps to specify they stick to the code.\n",
    "code_prompt = \"\"\"\n",
    "Write a Python function to calculate the factorial of a number.\n",
    "\"\"\"\n",
    "\n",
    "params = dict(temperature=1, top_p=1, max_tokens=1024)\n",
    "\n",
    "response, conversation_history = chat_with_openai_with_retry(code_prompt, \n",
    "                                                             conversation_history, \n",
    "                                                             model,\n",
    "                                                             **params)\n",
    "\n",
    "Markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1443dfb7-7326-4e84-9847-0032e1d4a0aa",
   "metadata": {},
   "source": [
    "## 7.2 Generate and run code\n",
    "\n",
    "openAI API does not have a built-in `code_execution` tool, so we'll write the required functionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "707cdfde-893e-48ac-9b6e-21670c5ab522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "\n",
    "class ExecuteCodeInput(BaseModel):\n",
    "    \"\"\"Define the Pydantic model for code execution\"\"\"\n",
    "    code: str = Field(description=\"The Python code to execute.\")\n",
    "\n",
    "\n",
    "def execute_code_locally(code: str) -> dict:\n",
    "    \"\"\"Define the function to execute Python code\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"python3\", \"-c\", code],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "        return result.stdout.strip()\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "code_exec_prompt = \"\"\"\n",
    "Calculate the sum of the first 14 prime numbers. Only consider the odd primes, and make sure you count them all.\n",
    "\"\"\"\n",
    "\n",
    "execute_code_tool = convert_pydantic_to_openai_function(ExecuteCodeInput)\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an assistant that writes Python code to solve mathematical problems.\"},\n",
    "        {\"role\": \"user\", \"content\": code_exec_prompt}],\n",
    "        tools=[{\"type\": \"function\", \"function\": execute_code_tool}]\n",
    ")\n",
    "\n",
    "# Step 1: Extract the tool call arguments\n",
    "tool_call = response.choices[0].message.tool_calls[0]\n",
    "arguments = tool_call.function.arguments\n",
    "\n",
    "# Step 2: Parse the JSON to get the Python code\n",
    "code_snippet = json.loads(arguments)['code']\n",
    "result = execute_code_locally(code_snippet)\n",
    "\n",
    "print(\"Result \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb83a40-6a46-4560-b376-aa8a3288156c",
   "metadata": {},
   "source": [
    "## 7.3 Explaining codeÂ¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c7bf5c46-c8c2-4575-b9e7-6956d954e16b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This is a Bash script designed to integrate Git repository information into your shell prompt. It aims to make your shell more interactive and informative by displaying useful Git details directly in your prompt.\n",
       "\n",
       "Here's a high-level explanation:\n",
       "\n",
       "**What it does:**\n",
       "\n",
       "* **Sets up a color scheme:**  The script defines color themes to customize the appearance of the Git information in your prompt. You can select from built-in themes or create your own custom theme.\n",
       "* **Retrieves Git status:**  It uses `git` commands to gather information about the current Git repository, including:\n",
       "    * **Branch name:**  Identifies the branch you're working on.\n",
       "    * **Upstream status:**  Indicates if your branch is ahead or behind the remote branch.\n",
       "    * **Staged and unstaged changes:**  Shows the number of staged or unstaged changes.\n",
       "    * **Other details:**  It can also include information about stashes, conflicts, and whether the working directory is clean.\n",
       "* **Customizes the prompt:**  The script uses the gathered Git information to dynamically modify your prompt, showing different elements depending on your settings and the current status of the repository.\n",
       "* **Virtual environment integration:**  It can display the name of any active virtual environment (e.g., from `venv`, `nvm`, `conda`) within your prompt.\n",
       "* **Window title integration:**  Optionally, the script can set the window title of your terminal to reflect the current Git repository information.\n",
       "* **Installs prompt commands:**  The script adds functions to your shell that will automatically update your prompt when you change directories or execute Git commands.\n",
       "\n",
       "**Why you would use it:**\n",
       "\n",
       "* **Contextual awareness:**  You can quickly see the current Git branch, upstream status, and changes without needing to run separate `git` commands. This keeps you in context and improves your workflow.\n",
       "* **Increased efficiency:** The visual cues in your prompt help you keep track of the repository's state, which can prevent errors or make it easier to navigate your project.\n",
       "* **Customizability:** You can choose a color scheme that suits your preferences, specify what Git information you want to see, and integrate the prompt with your existing shell settings.\n",
       "\n",
       "Overall, this script is a powerful tool for developers and Git users who want to make their shell prompt more interactive and informative, helping them work more efficiently and stay organized within their Git projects.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n",
    "\n",
    "conversation_history = [{\"role\": \"system\", \"content\": \"Act as a software developer.\"}]  # modified\n",
    "explain_prompt = f\"\"\"\n",
    "Please explain what this file does at a very high level. What is it, and why would I use it?\n",
    "\n",
    "```\n",
    "{file_contents}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "response, conversation_history = chat_with_openai_with_retry(explain_prompt, \n",
    "                                                             conversation_history, \n",
    "                                                             model)\n",
    "\n",
    "Markdown(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
